{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEW-SQZoCYbT"
      },
      "source": [
        "# Definición del problema"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se desea entrenar un modelo que sea capaz de 'completar' una palabra a medio escribir, o proponer una corrección para una palabra ya escrita en caso de que la misma se encuentre mal escrita.\n",
        "\n",
        "Se utilizará un algoritmo de 'hallar la palabra incorrecta' para determinar si una palabra está escrita incorrectamente, de acuerdo a un lexicón construido con palabras extraídas de la página web de la RAE (disponible en https://github.com/JorgeDuenasLerin/diccionario-espanol-txt, actualizado en Mayo 2024)\n",
        "\n",
        "Además, se construirá una matriz de probabilidad con las palabras extraídas para que las recomendaciones de completado y corrección se realicen en función de la frecuencia de utilización de las palabras. El sistema será capaz de realizar estas funciones en Español.\n",
        "\n",
        "Se utilizarán textos para entrenarlo."
      ],
      "metadata": {
        "id": "9FcXCHksCbGB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Estructura del modelo"
      ],
      "metadata": {
        "id": "_WMLpyK3Zci_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7SpmcbTWZhAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIQTHR1LCYbZ"
      },
      "source": [
        "# Datos de entrada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "NH6BNWm-CYbZ",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#Extrayendo el texto\n",
        "txt_file = \"text_dump.txt\"\n",
        "\n",
        "lines: list[str] = []\n",
        "# complete_text: str = \"\"\n",
        "with open(txt_file, \"r\") as file:\n",
        "  for line in file:\n",
        "    if line != \"\\n\":\n",
        "      lines.append(line)\n",
        "      # complete_text += line\n",
        "\n",
        "# print(len(lines))\n",
        "# print(complete_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "#Formando la data de entrada y salida esperada\n",
        "complete_text: str = \"\"\n",
        "regex_specials = \"-—?¿!¡:;.,\"\n",
        "regex_romans =  r\"\\b(?=[MDCLXVIΙ])M{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})([IΙ]X|[IΙ]V|V?[IΙ]{0,3})\\b\\.?\"\n",
        "\n",
        "\n",
        "for paragraph in lines[37 : 1852]:\n",
        "  clean_paragraph = \"\"\n",
        "  for char in paragraph:\n",
        "    if char not in regex_specials:\n",
        "      clean_paragraph += char\n",
        "  if re.match(regex_romans, paragraph):\n",
        "    complete_text += paragraph\n",
        "\n",
        "\n",
        "print(complete_text)\n",
        "\n",
        "#Construyendo la data de input, output y conjunto de tokens\n",
        "encoder_tokens: dict[str, int] = dict()\n",
        "decoder_tokens: dict[str, int] = dict()\n",
        "input_chars: list[str] = []\n",
        "output_chars: list[str] = []\n",
        "\n",
        "\n",
        "# input_chars.append(complete_text[0])\n",
        "\n",
        "#Llenar los inputs y outputs de manera escalonada\n",
        "for i in range(0, len(complete_text) - 1):\n",
        "  input_char = complete_text[i]\n",
        "\n",
        "  input_chars.append(input_char)\n",
        "  output_chars.append(complete_text[i + 1])\n",
        "\n",
        "  if input_char not in encoder_tokens:\n",
        "    index = len(encoder_tokens)\n",
        "    encoder_tokens[input_char] = index\n",
        "    decoder_tokens[index] = input_char\n",
        "\n",
        "\n",
        "# index = 0\n",
        "# for pair in zip(input_chars, output_chars):\n",
        "#   print(pair)\n",
        "#   index += 1\n",
        "#   if index >= 100:\n",
        "#     break\n",
        "# for key, value in zip(encoder_tokens.keys(), encoder_tokens.values()):\n",
        "#   print(f\"Key: {key} | Value: {value}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "rHfyF-Xmu0ur",
        "outputId": "e85465df-e5ca-44a6-ce5c-959a6fea13e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lo miró atentamente y dijo:\n",
            "Volví a dibujar.\n",
            "Mi amigo sonrió dulcemente, con indulgencia.\n",
            "III\n",
            "Me costó mucho tiempo comprender de dónde venía. El principito, que me hacía muchas preguntas, jamás parecía oír las mías. Fueron palabras pronunciadas al azar, las que poco a poco me revelaron todo. Así, cuando distinguió por vez primera mi avión (no dibujaré mi avión, por tratarse de un dibujo demasiado complicado para mí) me preguntó:\n",
            "Me sentía orgulloso al decirle que volaba. El entonces grit ó:\n",
            "Divisé una luz en el misterio de su presencia y le pregunté bruscamente:\n",
            "Imagínense cómo me intrigó esta semiconfidencia sobre los otros planetas. Me esforcé, pues, en saber algo más:\n",
            "Después de meditar silenciosamente me respondió:\n",
            "Mi amigo soltó una nueva carcajada.\n",
            "IV\n",
            "De esta manera supe una segunda cosa muy importante: su planeta de origen era apenas más grande que una casa.\n",
            "De tal manera, si les decimos: \"La prueba de que el principito ha existido está en que era un muchachito encantador, que reía y quería un cordero. Querer un cordero es prueba de que se existe\", las personas mayores se encogerán de hombros y nos dirán que somos unos niños. Pero si les decimos: \"el planeta de donde venía el principito era el asteroide B 612\", quedarán convencidas y no se preocuparán de hacer más preguntas. Son así. No hay por qué guardarles rencor. Los niños deben ser muy indulgentes con las personas mayores.\n",
            "V\n",
            "Cada día yo aprendía algo nuevo sobre el planeta, sobre la partida y sobre el viaje. Esto venía suavemente al azar de las reflexiones. De esta manera tuve conocimiento al tercer día, del drama de los baobabs.\n",
            "Le hice comprender al principito que los baobabs no son arbustos, sino árboles tan grandes como iglesias y que incluso si llevase consigo todo un rebaño de elefantes, el rebaño no lograría acabar con un solo baobab.\n",
            "Me contestó: \"¡Bueno! ¡Vamos!\" como si hablara de una evidencia. Me fue necesario un gran esfuerzo de inteligencia para comprender por mí mismo este problema.\n",
            "VI\n",
            "VII\n",
            "Confieso que no lo sabía. Estaba yo muy ocupado tratando de destornillar un perno demasiado apretado del motor; la avería comenzaba a parecerme cosa grave y la circunstancia de que se estuviera agotando mi provisión de agua, me hacía temer lo peor.\n",
            "Me miró estupefacto.\n",
            "Me miraba con mi martillo en la mano, los dedos llenos de grasa e inclinado sobre algo que le parecía muy feo.\n",
            "Me avergonzó un poco. Pero él, implacable, añadió:\n",
            "La noche había caído. Yo había soltado las herramientas y ya no importaban nada el martillo, el perno, la sed y la muerte. ¡Había en una estrella, en un planeta, el mío, la Tierra, un principito a quien consolar! Lo tomé en mis brazos y lo mecí diciéndole: \"la flor que tú quieres no corre peligro… te dibujaré un bozal para tu cordero y una armadura para la flor…te…\". No sabía qué decirle, cómo consolarle y hacer que tuviera nuevamente confianza en mí; me sentía torpe. ¡Es tan misterioso el país de las lágrimas!\n",
            "VIII\n",
            "La flor, que había trabajado con tanta precisión, dijo bostezando:\n",
            "La flor se interrumpió; había llegado allí en forma de semilla y no era posible que conociera otros mundos. Humillada por haberse dejado sorprender inventando una mentira tan ingenua, tosió dos o tres veces para atraerse la simpatía del principito.\n",
            "Insistió en su tos para darle al menos remordimientos.\n",
            "De esta manera el principito, a pesar de la buena voluntad de su amor, había llegado a dudar de ella. Había tomado en serio palabras sin importancia y se sentía desgraciado.\n",
            "IX\n",
            "Creo que el principito aprovechó la migración de una bandada de pájaros silvestres para su evasión. La mañana de la partida, puso en orden el planeta. Deshollinó cuidadosamente sus volcanes en actividad, de los cuales poseía dos, que le eran muy útiles para calentar el desayuno todas las mañanas.\n",
            "La flor tosió, pero no porque estuviera resfriada.\n",
            "La flor no quería que la viese llorar: era tan orgullosa...\n",
            "X\n",
            "Ignoraba que para los reyes el mundo está muy simplificado. Todos los hombres son súbditos.\n",
            "Los bostezos son para mí algo curioso. ¡Vamos, bosteza otra vez, te lo ordeno!\n",
            "Como el rey no respondiera nada, el principito vaciló primero y con un suspiro emprendió la marcha.\n",
            "XI\n",
            "XII\n",
            "XIII\n",
            "XIV\n",
            "Cuando llegó al planeta saludó respetuosamente al farolero:\n",
            "Luego se enjugó la frente con un pañuelo de cuadros rojos.\n",
            "Mientras el principito proseguía su viaje, se iba diciendo para sí: \"Este sería despreciado por los otros, por el rey, por el vanidoso, por el bebedor, por el hombre de negocios. Y, sin embargo, es el único que no me parece ridículo, quizás porque se ocupa de otra cosa y no de sí mismo. Lanzó un suspiro de pena y continuó diciéndose:\n",
            "Lo que el principito no se atrevía a confesarse, era que la causa por la cual lamentaba no quedarse en este bendito planeta se debía a las mil cuatrocientas cuarenta puestas de sol que podría disfrutar cada veinticuatro horas.\n",
            "XV\n",
            "Dirigió una mirada a su alrededor sobre el planeta del geógrafo; nunca había visto un planeta tan majestuoso.\n",
            "XVI\n",
            "Vistos desde lejos, hacían un espléndido efecto. Los movimientos de este ejército estaban regulados como los de un ballet de ópera. Primero venía el turno de los faroleros de Nueva Zelandia y de Australia. Encendían sus faroles y se iban a dormir. Después tocaba el turno en la danza a los faroleros de China y Siberia, que a su vez se perdían entre bastidores. Luego seguían los faroleros de Rusia y la India, después los de África y Europa y finalmente, los de América del Sur y América del Norte. Nunca se equivocaban en su orden de entrada en escena. Era grandioso.\n",
            "XVII\n",
            "Cuando se quiere ser ingenioso, sucede que se miente un poco. No he sido muy honesto al hablar de los faroleros y corro el riesgo de dar una falsa idea de nuestro planeta a los que no lo conocen.\n",
            "Los hombres ocupan muy poco lugar sobre la Tierra. Si los dos mil millones de habitantes que la pueblan se pusieran de pie y un poco apretados, como en un mitin, cabrían fácilmente en una plaza de veinte millas de largo por veinte de ancho. La humanidad podría amontonarse sobre el más pequeño islote del Pacífico.\n",
            "Las personas mayores no les creerán, seguramente, pues siempre se imaginan que ocupan mucho sitio. Se creen importantes como los baobabs. Les dirán, pues, que hagan el cálculo; eso les gustará ya que adoran las cifras. Pero no es necesario que pierdan el tiempo inútilmente, puesto que tienen confianza en mí.\n",
            "XVIII\n",
            "La flor, un día, había visto pasar una caravana.\n",
            "XIX\n",
            "XX\n",
            "XXI\n",
            "De esta manera el principito domesticó al zorro. Y cuando se fue acercando el día de la partida:\n",
            "Las rosas se sentían molestas oyendo al principito, que continuó diciéndoles:\n",
            "XXII\n",
            "XXIII\n",
            "XXIV\n",
            "Después de dos horas de caminar en silencio, cayó la noche y las estrellas comenzaron a brillar.\n",
            "Me quedé sorprendido al comprender súbitamente ese misterioso resplandor de la arena. Cuando yo era niño vivía en una casa antigua en la que, según la leyenda, había un tesoro escondido. Sin duda que nadie supo jamás descubrirlo y quizás nadie lo buscó, pero parecía toda encantada por ese tesoro.\n",
            "Mi casa ocultaba un secreto en el fondo de su corazón...\n",
            "Como el principito se dormía, lo tomé en mis brazos y me puse nuevamente en camino. Me sentía emocionado llevando aquel frágil tesoro, y me parecía que nada más frágil había sobre la Tierra.\n",
            "Miraba a la luz de la luna aquella frente pálida, aquellos ojos cerrados, los cabellos agitados por el viento y me decía: \"lo que veo es sólo la corteza; lo más importante es invisible... \"\n",
            "Como sus labios entreabiertos esbozaron una sonrisa, me dije: \"Lo que más me emociona de este principito dormido es su fidelidad a una flor, es la imagen de la rosa que resplandece en él como la llama de una lámpara, incluso cuando duerme... \" Y lo sentí más frágil aún. Pensaba que a las lámparas hay que protegerlas: una racha de viento puede apagarlas...\n",
            "Continué caminando y al rayar el alba descubrí el pozo.\n",
            "XXV\n",
            "Lentamente subí el cubo hasta el brocal donde lo dejé bien seguro. En mis oídos sonaba aún el canto de la roldana y veía temblar al sol en el agua agitada.\n",
            "Levanté el balde hasta sus labios y el principito bebió con los ojos cerrados. Todo era bello como una fiesta. Aquella agua era algo más que un alimento. Había nacido del caminar bajo las estrellas, del canto de la roldana, del esfuerzo de mis brazos. Era como un regalo para el corazón. Cuando yo era niño, las luces del árbol de Navidad, la música de la misa de medianoche, la dulzura de las sonrisas, daban su resplandor a mi regalo de Navidad.\n",
            "XXVI\n",
            "Me detuve con el corazón oprimido, siempre sin comprender.\n",
            "Dirigí la mirada hacia el pie del muro e instintivamente di un brinco. Una serpiente de esas amarillas que matan a una persona en menos de treinta segundos, se erguía en dirección al principito.\n",
            "Llegué junto al muro a tiempo de recibir en mis brazos a mi principito, que estaba blanco como la nieve.\n",
            "Le quité su eterna bufanda de oro, le humedecí las sienes y le di de beber, sin atreverme a hacerle pregunta alguna. Me miró gravemente rodeándome el cuello con sus brazos. Sentí latir su corazón, como el de un pajarillo que muere a tiros de carabina.\n",
            "Luego, con melancolía:\n",
            "Me daba cuenta de que algo extraordinario pasaba en aquellos momentos. Estreché al principito entre mis brazos como sí fuera un niño pequeño, y no obstante, me pareció que descendía en picada hacia un abismo sin que fuera posible hacer nada para retenerlo.\n",
            "Lo había tenido, sin duda, pero sonrió con dulzura:\n",
            "Me quedé de nuevo helado por un sentimiento de algo irreparable. Comprendí que no podía soportar la idea de no volver a oír nunca más su risa. Era para mí como una fuente en el desierto.\n",
            "La gente tiene estrellas que no son las mismas. Para los que viajan, las estrellas son guías; para otros sólo son pequeñas lucecitas. Para los sabios las estrellas son problemas. Para mi hombre de negocios, eran oro. Pero todas esas estrellas se callan. Tú tendrás estrellas como nadie ha tenido...\n",
            "Me cogió de la mano y todavía se atormentó:\n",
            "Me senté, ya no podía mantenerme en pie.\n",
            "Vaciló todavía un instante, luego se levantó y dio un paso. Yo no pude moverme.\n",
            "Luego cayó lentamente como cae un árbol, sin hacer el menor ruido a causa de la arena.\n",
            "XXVII\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Creando la matriz de caracteres\n",
        "\n",
        "input_size = len(input_chars)\n",
        "token_size = len(encoder_tokens)\n",
        "\n",
        "input_data = np.zeros((input_size, 1,token_size), dtype= \"float32\")\n",
        "# decoder_input_data = np.zeros((input_size, token_size), dtype= \"float32\")\n",
        "output_data = np.zeros((input_size, 1,token_size), dtype= \"float32\")\n",
        "\n",
        "\n",
        "for i, (input, output) in enumerate(zip(input_chars, output_chars)):\n",
        "  input_data[i, 0,encoder_tokens[input]] = 1.0\n",
        "  output_data[i, 0,encoder_tokens[output]] = 1.0"
      ],
      "metadata": {
        "id": "884DUN0lahf9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0VZpRHJCYbX"
      },
      "source": [
        "# Construción del modelo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import initializers\n",
        "\n",
        "neurons = 800"
      ],
      "metadata": {
        "id": "ttqLZYb3o2ad"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight_initializer = initializers.RandomNormal(mean= 0.01, stddev= 0.08)\n",
        "\n",
        "encoder_inputs = keras.Input(shape= (None, token_size))\n",
        "encoder = keras.layers.LSTM(neurons, return_state= True, kernel_initializer= weight_initializer)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "decoder_inputs = keras.Input(shape= (None, token_size))\n",
        "\n",
        "decoder_lstm = keras.layers.LSTM(neurons, return_sequences= True, return_state= True, kernel_initializer= weight_initializer)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state= encoder_states)\n",
        "decoder_dense = keras.layers.Dense(token_size, activation= \"softmax\", kernel_initializer= weight_initializer )\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.compile(optimizer= \"rmsprop\", loss= \"categorical_crossentropy\", metrics= [\"accuracy\"])"
      ],
      "metadata": {
        "id": "O3rey1t9pyk1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8irePKGLCYba"
      },
      "source": [
        "# Entrenamiento del modelo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit([input_data, input_data], output_data, batch_size= 128, epochs= 30, validation_split= 0.3)\n",
        "\n",
        "model.save(\"s2s-Archivo.keras\")"
      ],
      "metadata": {
        "id": "owd6bjCosH-2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QD6SIafXCYbb"
      },
      "source": [
        "# Evaluación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "z7UV-B3eCYbb",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "model = keras.models.load_model(\"s2s-Archivo.keras\")\n",
        "\n",
        "encoder_inputs = model.input[0]\n",
        "encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output\n",
        "encoder_states = [state_h_enc, state_c_enc]\n",
        "encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_inputs = model.input[1]\n",
        "decoder_state_inputs = [keras.Input(shape= (neurons,)), keras.Input(shape= (neurons,))]\n",
        "decoder_lstm = model.layers[3]\n",
        "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(decoder_inputs, initial_state= decoder_state_inputs)\n",
        "decoder_states = [state_h_dec, state_c_dec]\n",
        "decoder_dense = model.layers[4]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = keras.Model([decoder_inputs] + decoder_state_inputs, [decoder_outputs] + decoder_states)\n",
        "\n",
        "def decode_word(input_char: str) -> str:\n",
        "  data = np.zeros((1, 1, token_size))\n",
        "  data[0, 0, encoder_tokens[input_char]] = 1.0\n",
        "\n",
        "  state_value = encoder_model.predict(data)\n",
        "\n",
        "  target_sequence = np.zeros((1, 1, token_size))\n",
        "  target_sequence[0,0, encoder_tokens[input_char]] = 1.0\n",
        "\n",
        "  decoded_word = input_char\n",
        "  sampled_char = \"\"\n",
        "  failsafe = 0\n",
        "  while sampled_char != \" \" and failsafe < 25:\n",
        "    failsafe += 1\n",
        "    output_tokens, h, c = decoder_model.predict([target_sequence] + state_value)\n",
        "\n",
        "    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "    sampled_char = decoder_tokens[sampled_token_index]\n",
        "    decoded_word += sampled_char\n",
        "\n",
        "    target_sequence = np.zeros((1, 1, token_size))\n",
        "    target_sequence[0, 0, sampled_token_index] = 1.0\n",
        "\n",
        "    state_value = [h, c]\n",
        "\n",
        "  return decoded_word\n",
        "\n",
        "\n",
        "# sequence = input_data[1 : 2]\n",
        "# print(decoder_tokens[np.argmax(input_data[1, -1, :])])\n",
        "\n",
        "# # Define the shape of the decoder input explicitly\n",
        "# decoder_input_shape = (sequence.shape[0], sequence.shape[1], token_size)\n",
        "# # Assuming token_size is defined in your code\n",
        "\n",
        "# # Create the decoder input with the defined shape\n",
        "# decoder_input = output_data[1 : 2]\n",
        "# i = 0\n",
        "# decoded_char = \"\"\n",
        "# while decoded_char != \" \" and i < 25:\n",
        "#   i += 1\n",
        "#   # Now, predict using the model\n",
        "#   output_tokens = model.predict([sequence, decoder_input])\n",
        "\n",
        "#   token_index = np.argmax(output_tokens[0, -1, :])\n",
        "\n",
        "#   decoded_char = decoder_tokens[token_index]\n",
        "\n",
        "#   print(decoded_char)\n",
        "\n",
        "#   sequence[0, 0, token_index] = 1.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_word = decode_word(\"h\")\n",
        "\n",
        "print(predicted_word)"
      ],
      "metadata": {
        "id": "59EVitGEgbDr",
        "outputId": "e6b6f5ba-d827-467f-f988-0353a4300756",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "haue \n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}