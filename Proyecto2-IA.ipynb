{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEW-SQZoCYbT"
      },
      "source": [
        "# Definición del problema"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se desea entrenar un modelo que sea capaz de 'completar' una palabra a medio escribir, o proponer una corrección para una palabra ya escrita en caso de que la misma se encuentre mal escrita.\n",
        "\n",
        "Se utilizará un algoritmo de 'hallar la palabra incorrecta' para determinar si una palabra está escrita incorrectamente, de acuerdo a un lexicón construido con palabras extraídas de la página web de la RAE (disponible en https://github.com/JorgeDuenasLerin/diccionario-espanol-txt, actualizado en Mayo 2024)\n",
        "\n",
        "Además, se construirá una matriz de probabilidad con las palabras extraídas para que las recomendaciones de completado y corrección se realicen en función de la frecuencia de utilización de las palabras. El sistema será capaz de realizar estas funciones en Español.\n",
        "\n",
        "Se utilizarán textos para entrenarlo."
      ],
      "metadata": {
        "id": "9FcXCHksCbGB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0VZpRHJCYbX"
      },
      "source": [
        "# Estructura del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "ou1izJryCYbY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importacion de Librerias"
      ],
      "metadata": {
        "id": "Zxrx98HjngrH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import keras as kr\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM"
      ],
      "metadata": {
        "id": "R0_I7z3pnlT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIQTHR1LCYbZ"
      },
      "source": [
        "# Datos de entrada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "NH6BNWm-CYbZ"
      },
      "outputs": [],
      "source": [
        "# Extract text from Spanish book converted into txt format\n",
        "txt_file = \"text_dump.txt\"\n",
        "\n",
        "lines: list[str] = []\n",
        "with open(txt_file, 'r', encoding=\"UTF-8\") as file:\n",
        "  for line in file:\n",
        "    if line != \"\\n\": #Do not include empty lines in text digest\n",
        "      lines.append(line)\n",
        "\n",
        "#As of this point, 'lines' variable should hold the full length of the txt file\n",
        "print(\"Total line count:\", len(lines))\n",
        "print(\"Total character count:\", sum([len(item) for item in lines]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract used vocabulary and construct encode/decode dictionaries\n",
        "full_txt_str: str = \"\"\n",
        "vocab: list[str] = []\n",
        "acc = 0 #Used to print partial progress of the operation\n",
        "big_acc = 1 #Same as above\n",
        "for paragraph in lines[31:14346]: #Do not consider index, acknowledgements, appendix, etc etc (i.e. only consider main story block for training)\n",
        "  full_txt_str += paragraph\n",
        "  for char in paragraph:\n",
        "    if char not in vocab:\n",
        "      vocab.append(char)\n",
        "  #Print partial progress\n",
        "  acc += 1\n",
        "  if (acc >= (14346-31)*0.2):\n",
        "    print(f\"Vocabulary {20*big_acc}% built\")\n",
        "    acc = 0\n",
        "    big_acc += 1\n",
        "vocab.sort()\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "encode_keys:dict[str,int] = {}\n",
        "decode_keys: dict[int, str] = {}\n",
        "for index, char in enumerate(vocab):\n",
        "  encode_keys[char] = index\n",
        "  decode_keys[index] = char\n",
        "\n",
        "#As of this point, the full vocabulary should be indexed\n",
        "print(\"Total unique characters:\", vocab_size)\n",
        "print(\"Total encode/decode dictionary keys:\", len(encode_keys.keys()),\"|\", len(decode_keys.keys()))\n",
        "print(\"Key list\\n\",encode_keys.keys())"
      ],
      "metadata": {
        "id": "TzR18gLGE5B0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct input/output data for LSTM network\n",
        "input_txt = full_txt_str[:-1]\n",
        "output_txt = full_txt_str[1:]\n",
        "train_data_list: list[np.ndarray] = []\n",
        "train_tag_list: list[np.ndarray] = []\n",
        "acc = 0 #Used to print partial progress of the operation\n",
        "big_acc = 1 #Same as above\n",
        "\n",
        "#The LSTM network will recieve a 'card' (vector) containing one square per vocab character. The desired character will be 'marked' as 1.0 whilst all others remain zero\n",
        "for item in zip(input_txt, output_txt):\n",
        "  data:np.ndarray = np.zeros(vocab_size, dtype=np.float64)\n",
        "  tag:np.ndarray = np.zeros(vocab_size, dtype=np.float64)\n",
        "  data[encode_keys[item[0]]] = 1.0\n",
        "  tag[encode_keys[item[1]]] = 1.0\n",
        "\n",
        "  train_data_list.append(data)\n",
        "  train_tag_list.append(tag)\n",
        "  #Print partial progress\n",
        "  acc += 1\n",
        "  if (acc >= len(input_txt)*0.2):\n",
        "    print(f\"IO data {10*big_acc}% built\")\n",
        "    acc = 0\n",
        "    big_acc += 1\n",
        "\n",
        "#As of this point, the full set of training data has been built\n",
        "train_data = np.array(train_data_list)\n",
        "train_tag = np.array(train_tag_list)\n",
        "print(\"Train shape:\", train_data.shape)\n",
        "print(\"Tag shape:\", train_tag.shape)"
      ],
      "metadata": {
        "id": "w3qZR9S7khoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Construcción del modelo"
      ],
      "metadata": {
        "id": "OPUwYDmX97It"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vect_model = Sequential()\n",
        "vect_model.add(kr.Input(shape=(vocab_size,)))\n",
        "vect_model.add(LSTM(100, return_sequences=True)) #Add dropout?\n",
        "vect_model.add(Dense(vocab_size, activation=\"softmax\"))\n",
        "\n",
        "vect_model.compile(loss=\"categorical_crossentropy\",optimizer=\"rmsprop\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "uBjJAtCr996s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8irePKGLCYba"
      },
      "source": [
        "# Entrenamiento del modelo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vect_model.fit(train_data, train_tag, epochs = 10)"
      ],
      "metadata": {
        "id": "62DcwYfoECfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QD6SIafXCYbb"
      },
      "source": [
        "# Evaluación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "z7UV-B3eCYbb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}