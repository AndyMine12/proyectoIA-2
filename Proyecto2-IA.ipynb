{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEW-SQZoCYbT"
      },
      "source": [
        "# Definición del problema"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se desea entrenar un modelo que sea capaz de 'completar' una palabra a medio escribir, o proponer una corrección para una palabra ya escrita en caso de que la misma se encuentre mal escrita.\n",
        "\n",
        "Se utilizará un algoritmo de 'hallar la palabra incorrecta' para determinar si una palabra está escrita incorrectamente, de acuerdo a un lexicón construido con palabras extraídas de la página web de la RAE (disponible en https://github.com/JorgeDuenasLerin/diccionario-espanol-txt, actualizado en Mayo 2024)\n",
        "\n",
        "Además, se construirá una matriz de probabilidad con las palabras extraídas para que las recomendaciones de completado y corrección se realicen en función de la frecuencia de utilización de las palabras. El sistema será capaz de realizar estas funciones en Español.\n",
        "\n",
        "Se utilizarán textos para entrenarlo."
      ],
      "metadata": {
        "id": "9FcXCHksCbGB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Estructura del modelo"
      ],
      "metadata": {
        "id": "_WMLpyK3Zci_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7SpmcbTWZhAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIQTHR1LCYbZ"
      },
      "source": [
        "# Datos de entrada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "NH6BNWm-CYbZ",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#Extrayendo el texto\n",
        "txt_file = \"text_dump.txt\"\n",
        "\n",
        "lines: list[str] = []\n",
        "# complete_text: str = \"\"\n",
        "with open(txt_file, \"r\") as file:\n",
        "  for line in file:\n",
        "    if line != \"\\n\":\n",
        "      lines.append(line)\n",
        "      # complete_text += line\n",
        "\n",
        "# print(len(lines))\n",
        "# print(complete_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Formando la data de entrada y salida esperada\n",
        "complete_text: str = \"\"\n",
        "\n",
        "for paragraph in lines[31 : 14346]:\n",
        "  complete_text += paragraph\n",
        "\n",
        "#Construyendo la data de input, output y conjunto de tokens\n",
        "encoder_tokens: dict[str, int] = dict()\n",
        "decoder_tokens: dict[str, int] = dict()\n",
        "# input_chars: list[str] = []\n",
        "# output_chars: list[str] = []\n",
        "input_words: list[str] = []\n",
        "output_words: list[str] = []\n",
        "\n",
        "\n",
        "# input_chars.append(complete_text[0])\n",
        "\n",
        "#Llenar los inputs y outputs de manera escalonada\n",
        "input_word = \"\"\n",
        "# output_word = \"\"\n",
        "current_char = \"\"\n",
        "index = 0\n",
        "while current_char != \"\\n\" and current_char != \" \":\n",
        "  current_char = complete_text[index]\n",
        "  input_word += current_char\n",
        "  index += 1\n",
        "\n",
        "  if current_char not in encoder_tokens:\n",
        "    tk_i = len(encoder_tokens)\n",
        "    encoder_tokens[current_char] = tk_i\n",
        "    decoder_tokens[tk_i] = current_char\n",
        "\n",
        "input_words.append(input_word)\n",
        "input_word = \"\"\n",
        "for i in range(index, len(complete_text) - 1):\n",
        "  current_char = complete_text[i]\n",
        "\n",
        "  if current_char != \" \" and current_char != \"\\n\":\n",
        "    input_word += current_char\n",
        "  else:\n",
        "    input_words.append(input_word + current_char)\n",
        "    output_words.append(input_word + current_char)\n",
        "    input_word = \"\"\n",
        "\n",
        "  if current_char not in encoder_tokens:\n",
        "    tk_i = len(encoder_tokens)\n",
        "    encoder_tokens[current_char] = tk_i\n",
        "    decoder_tokens[tk_i] = current_char\n",
        "\n",
        "output_words.append(input_word)\n",
        "\n",
        "# for i in range(0, len(complete_text) - 1):\n",
        "#   input_char = complete_text[i]\n",
        "\n",
        "#   input_chars.append(input_char)\n",
        "#   output_chars.append(complete_text[i + 1])\n",
        "\n",
        "#   if input_char not in encoder_tokens:\n",
        "#     index = len(encoder_tokens)\n",
        "#     encoder_tokens[input_char] = index\n",
        "#     decoder_tokens[index] = input_char\n",
        "\n",
        "\n",
        "# index = 0\n",
        "# for pair in zip(input_chars, output_chars):\n",
        "#   print(pair)\n",
        "#   index += 1\n",
        "#   if index >= 100:\n",
        "#     break\n",
        "# for key, value in zip(encoder_tokens.keys(), encoder_tokens.values()):\n",
        "#   print(f\"Key: {key} | Value: {value}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "rHfyF-Xmu0ur"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Creando la matriz de caracteres\n",
        "\n",
        "input_size = len(input_words)\n",
        "max_input_word_length = max([len(word) for word in input_words])\n",
        "max_output_word_length = max([len(word) for word in output_words])\n",
        "token_size = len(encoder_tokens)\n",
        "\n",
        "encoder_input_data = np.zeros((input_size, max_input_word_length, token_size), dtype= \"float32\")\n",
        "decoder_input_data = np.zeros((input_size, max_output_word_length, token_size), dtype= \"float32\")\n",
        "decoder_output_data = np.zeros((input_size, max_output_word_length, token_size), dtype= \"float32\")\n",
        "\n",
        "for i, (input, output) in enumerate(zip(input_words, output_words)):\n",
        "  for j, char in enumerate(input):\n",
        "    encoder_input_data[i, j, encoder_tokens[char]] = 1.0 #Se guardan las palabras de input\n",
        "\n",
        "  for j, char in enumerate(output):\n",
        "    decoder_input_data[i, j, encoder_tokens[char]] = 1.0 #Se guarda la siguiente palabra esperada para cada input\n",
        "    if j > 0:\n",
        "      decoder_output_data[i, j - 1, encoder_tokens[char]] = 1.0 #Se guarda en la salida del decodificador la palabra esperada en tiempo t + 1\n",
        "\n",
        "# for i, (input, output) in enumerate(zip(input_chars, output_chars)):\n",
        "#   input_data[i, 0,encoder_tokens[input]] = 1.0\n",
        "#   output_data[i, 0,encoder_tokens[output]] = 1.0"
      ],
      "metadata": {
        "id": "884DUN0lahf9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(encoder_input_data[0])\n",
        "# for n in decoder_output_data[0]:\n",
        "#   print(n)\n",
        "# print(decoder_output_data[0])"
      ],
      "metadata": {
        "id": "zWNTm-dPIWzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0VZpRHJCYbX"
      },
      "source": [
        "# Construción del modelo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import initializers\n",
        "\n",
        "neurons = 400"
      ],
      "metadata": {
        "id": "ttqLZYb3o2ad"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight_initializer = initializers.RandomNormal(mean= 0.05, stddev= 0.04)\n",
        "\n",
        "encoder_inputs = keras.Input(shape= (None, token_size))\n",
        "encoder = keras.layers.LSTM(neurons, return_state= True, kernel_initializer= weight_initializer)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "decoder_inputs = keras.Input(shape= (None, token_size))\n",
        "\n",
        "decoder_lstm = keras.layers.LSTM(neurons, return_sequences= True, return_state= True, kernel_initializer= weight_initializer)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state= encoder_states)\n",
        "decoder_dense = keras.layers.Dense(token_size, activation= \"softmax\", kernel_initializer= weight_initializer )\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.compile(optimizer= \"rmsprop\", loss= \"categorical_crossentropy\", metrics= [\"accuracy\"])"
      ],
      "metadata": {
        "id": "O3rey1t9pyk1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "258b40a2-2fa7-4e23-bd34-ca3c0e51eb66"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'initializers' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-fd0554bee734>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mweight_initializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstddev\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m0.04\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mencoder_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneurons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_state\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_initializer\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mweight_initializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'initializers' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8irePKGLCYba"
      },
      "source": [
        "# Entrenamiento del modelo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit([encoder_input_data, decoder_input_data], decoder_output_data, batch_size= 512, epochs= 5, validation_split= 0.3)\n",
        "\n",
        "model.save(\"s2s-Archivo.keras\")"
      ],
      "metadata": {
        "id": "owd6bjCosH-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QD6SIafXCYbb"
      },
      "source": [
        "# Evaluación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "z7UV-B3eCYbb",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "model = keras.models.load_model(\"s2s-Archivo.keras\")\n",
        "\n",
        "encoder_inputs = model.input[0]\n",
        "encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output\n",
        "encoder_states = [state_h_enc, state_c_enc]\n",
        "encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_inputs = model.input[1]\n",
        "decoder_state_inputs = [keras.Input(shape= (neurons,)), keras.Input(shape= (neurons,))]\n",
        "decoder_lstm = model.layers[3]\n",
        "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(decoder_inputs, initial_state= decoder_state_inputs)\n",
        "decoder_states = [state_h_dec, state_c_dec]\n",
        "decoder_dense = model.layers[4]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = keras.Model([decoder_inputs] + decoder_state_inputs, [decoder_outputs] + decoder_states)\n",
        "\n",
        "def decode_word(input_char: str) -> str:\n",
        "  data = np.zeros((1, 1, token_size))\n",
        "  data[0, 0, encoder_tokens[input_char]] = 1.0\n",
        "\n",
        "  state_value = encoder_model.predict(data)\n",
        "\n",
        "  target_sequence = np.zeros((1, 1, token_size))\n",
        "  target_sequence[0,0, encoder_tokens[input_char]] = 1.0\n",
        "\n",
        "  decoded_word = input_char\n",
        "  sampled_char = \"\"\n",
        "  failsafe = 0\n",
        "  while sampled_char != \" \" and failsafe < 25:\n",
        "    failsafe += 1\n",
        "    output_tokens, h, c = decoder_model.predict([target_sequence] + state_value)\n",
        "\n",
        "    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "    sampled_char = decoder_tokens[sampled_token_index]\n",
        "    decoded_word += sampled_char\n",
        "\n",
        "    target_sequence = np.zeros((1, 1, token_size))\n",
        "    target_sequence[0, 0, sampled_token_index] = 1.0\n",
        "\n",
        "    state_value = [h, c]\n",
        "\n",
        "  return decoded_word\n",
        "\n",
        "\n",
        "# sequence = input_data[1 : 2]\n",
        "# print(decoder_tokens[np.argmax(input_data[1, -1, :])])\n",
        "\n",
        "# # Define the shape of the decoder input explicitly\n",
        "# decoder_input_shape = (sequence.shape[0], sequence.shape[1], token_size)\n",
        "# # Assuming token_size is defined in your code\n",
        "\n",
        "# # Create the decoder input with the defined shape\n",
        "# decoder_input = output_data[1 : 2]\n",
        "# i = 0\n",
        "# decoded_char = \"\"\n",
        "# while decoded_char != \" \" and i < 25:\n",
        "#   i += 1\n",
        "#   # Now, predict using the model\n",
        "#   output_tokens = model.predict([sequence, decoder_input])\n",
        "\n",
        "#   token_index = np.argmax(output_tokens[0, -1, :])\n",
        "\n",
        "#   decoded_char = decoder_tokens[token_index]\n",
        "\n",
        "#   print(decoded_char)\n",
        "\n",
        "#   sequence[0, 0, token_index] = 1.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_word = decode_word(\"K\")\n",
        "\n",
        "print(predicted_word)"
      ],
      "metadata": {
        "id": "59EVitGEgbDr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "164c4a87-2eed-49a2-997b-3b690a97f776"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "jooaaaaaaaaaaaaaaa \n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}