{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEW-SQZoCYbT"
      },
      "source": [
        "# Definición del problema"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se desea entrenar un modelo que sea capaz de 'completar' una palabra a medio escribir, o proponer una corrección para una palabra ya escrita en caso de que la misma se encuentre mal escrita.\n",
        "\n",
        "Se utilizará un algoritmo de 'hallar la palabra incorrecta' para determinar si una palabra está escrita incorrectamente, de acuerdo a un lexicón construido con palabras extraídas de la página web de la RAE (disponible en https://github.com/JorgeDuenasLerin/diccionario-espanol-txt, actualizado en Mayo 2024)\n",
        "\n",
        "Además, se construirá una matriz de probabilidad con las palabras extraídas para que las recomendaciones de completado y corrección se realicen en función de la frecuencia de utilización de las palabras. El sistema será capaz de realizar estas funciones en Español.\n",
        "\n",
        "Se utilizarán textos para entrenarlo."
      ],
      "metadata": {
        "id": "9FcXCHksCbGB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Estructura del modelo"
      ],
      "metadata": {
        "id": "_WMLpyK3Zci_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7SpmcbTWZhAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIQTHR1LCYbZ"
      },
      "source": [
        "# Datos de entrada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "NH6BNWm-CYbZ",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#Extrayendo el texto\n",
        "txt_file = \"text_dump.txt\"\n",
        "\n",
        "lines: list[str] = []\n",
        "# complete_text: str = \"\"\n",
        "with open(txt_file, \"r\") as file:\n",
        "  for line in file:\n",
        "    if line != \"\\n\":\n",
        "      lines.append(line)\n",
        "      # complete_text += line\n",
        "\n",
        "# print(len(lines))\n",
        "# print(complete_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Formando la data de entrada y salida esperada\n",
        "complete_text: str = \"\"\n",
        "\n",
        "for paragraph in lines[31 : 14346]:\n",
        "  complete_text += paragraph\n",
        "\n",
        "#Construyendo la data de input, output y conjunto de tokens\n",
        "encoder_tokens: dict[str, int] = dict()\n",
        "decoder_tokens: dict[str, int] = dict()\n",
        "input_chars: list[str] = []\n",
        "output_chars: list[str] = []\n",
        "\n",
        "\n",
        "# input_chars.append(complete_text[0])\n",
        "\n",
        "#Llenar los inputs y outputs de manera escalonada\n",
        "for i in range(0, len(complete_text) - 1):\n",
        "  input_char = complete_text[i]\n",
        "\n",
        "  input_chars.append(input_char)\n",
        "  output_chars.append(complete_text[i + 1])\n",
        "\n",
        "  if input_char not in encoder_tokens:\n",
        "    index = len(encoder_tokens)\n",
        "    encoder_tokens[input_char] = index\n",
        "    decoder_tokens[index] = input_char\n",
        "\n",
        "\n",
        "# index = 0\n",
        "# for pair in zip(input_chars, output_chars):\n",
        "#   print(pair)\n",
        "#   index += 1\n",
        "#   if index >= 100:\n",
        "#     break\n",
        "# for key, value in zip(encoder_tokens.keys(), encoder_tokens.values()):\n",
        "#   print(f\"Key: {key} | Value: {value}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "rHfyF-Xmu0ur"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Creando la matriz de caracteres\n",
        "\n",
        "input_size = len(input_chars)\n",
        "token_size = len(encoder_tokens)\n",
        "\n",
        "input_data = np.zeros((input_size, 1,token_size), dtype= \"float32\")\n",
        "# decoder_input_data = np.zeros((input_size, token_size), dtype= \"float32\")\n",
        "output_data = np.zeros((input_size, 1,token_size), dtype= \"float32\")\n",
        "\n",
        "\n",
        "for i, (input, output) in enumerate(zip(input_chars, output_chars)):\n",
        "  input_data[i, 0,encoder_tokens[input]] = 1.0\n",
        "  output_data[i, 0,encoder_tokens[output]] = 1.0"
      ],
      "metadata": {
        "id": "884DUN0lahf9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0VZpRHJCYbX"
      },
      "source": [
        "# Construción del modelo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "neurons = 256"
      ],
      "metadata": {
        "id": "ttqLZYb3o2ad"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_inputs = keras.Input(shape= (None, token_size))\n",
        "encoder = keras.layers.LSTM(neurons, return_state= True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "decoder_inputs = keras.Input(shape= (None, token_size))\n",
        "\n",
        "decoder_lstm = keras.layers.LSTM(neurons, return_sequences= True, return_state= True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state= encoder_states)\n",
        "decoder_dense = keras.layers.Dense(token_size, activation= \"softmax\")\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.compile(optimizer= \"rmsprop\", loss= \"categorical_crossentropy\", metrics= [\"accuracy\"])"
      ],
      "metadata": {
        "id": "O3rey1t9pyk1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8irePKGLCYba"
      },
      "source": [
        "# Entrenamiento del modelo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit([input_data, input_data], output_data, batch_size= 512, epochs= 20, validation_split= 0.3)\n",
        "\n",
        "model.save(\"s2s-Archivo\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owd6bjCosH-2",
        "outputId": "ebbdf6b3-db6e-4422-96fd-c4d27465fb6c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m6275/6275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 52ms/step - accuracy: 0.9973 - loss: 0.0220 - val_accuracy: 0.9997 - val_loss: 0.0015\n",
            "Epoch 2/5\n",
            "\u001b[1m6275/6275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 51ms/step - accuracy: 0.9999 - loss: 9.8986e-04 - val_accuracy: 0.9999 - val_loss: 8.7067e-04\n",
            "Epoch 3/5\n",
            "\u001b[1m6275/6275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m381s\u001b[0m 51ms/step - accuracy: 0.9999 - loss: 5.9922e-04 - val_accuracy: 0.9999 - val_loss: 6.3321e-04\n",
            "Epoch 4/5\n",
            "\u001b[1m6275/6275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m392s\u001b[0m 53ms/step - accuracy: 0.9999 - loss: 4.6923e-04 - val_accuracy: 1.0000 - val_loss: 4.9346e-04\n",
            "Epoch 5/5\n",
            "\u001b[1m6275/6275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m374s\u001b[0m 52ms/step - accuracy: 1.0000 - loss: 3.7884e-04 - val_accuracy: 1.0000 - val_loss: 3.9570e-04\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x79a3f01f6e00>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QD6SIafXCYbb"
      },
      "source": [
        "# Evaluación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "z7UV-B3eCYbb",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "model - keras.models.load_model(\"s2s-Archivo\")\n",
        "\n",
        "encoder_inputs = model.input[0]\n",
        "encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output\n",
        "encoder_states = [state_h_enc, state_c_enc]\n",
        "encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_inputs = model.input[1]\n",
        "decoder_state_inputs = [keras.Input(shape= (neurons,)), keras.Input(shape= (neurons,))]\n",
        "decoder_lstm = model.layers[3]\n",
        "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(decoder_inputs, initial_state= decoder_state_inputs)\n",
        "decoder_states = [state_h_dec, state_c_dec]\n",
        "decoder_dense = model.layers[4]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = keras.Model([decoder_inputs] + decoder_state_inputs, [decoder_outputs] + decoder_states)\n",
        "\n",
        "def decode_char(input_char: str) -> str:\n",
        "  data = np.zeros((1, 1, token_size))\n",
        "  data[0, 0, encoder_tokens[input_char]]\n",
        "\n",
        "  state_value = encoder_model.predict(data)\n",
        "\n",
        "  target_sequence = np.zeros((1, 1, token_size))\n",
        "  #target_sequence[0,0, encoder_tokens[\" \"]]\n",
        "\n",
        "  decoded_word = \"\"\n",
        "  sampled_char = \"\"\n",
        "  failsafe = 0\n",
        "  while sampled_char != \" \" and failsafe < 25:\n",
        "    failsafe += 1\n",
        "    output_tokens, h, c = decoder_model.predict([target_sequence] + state_value)\n",
        "\n",
        "    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "    sampled_char = decoder_tokens[sampled_token_index]\n",
        "    decoded_word += sampled_char\n",
        "\n",
        "    target_sequence = np.zeros((1, 1, token_size))\n",
        "    target_sequence[0, 0, sampled_token_index] = 1.0\n",
        "\n",
        "    state_value = [h, c]\n",
        "\n",
        "  return decoded_word\n",
        "\n",
        "\n",
        "# sequence = input_data[1 : 2]\n",
        "# print(decoder_tokens[np.argmax(input_data[1, -1, :])])\n",
        "\n",
        "# # Define the shape of the decoder input explicitly\n",
        "# decoder_input_shape = (sequence.shape[0], sequence.shape[1], token_size)\n",
        "# # Assuming token_size is defined in your code\n",
        "\n",
        "# # Create the decoder input with the defined shape\n",
        "# decoder_input = output_data[1 : 2]\n",
        "# i = 0\n",
        "# decoded_char = \"\"\n",
        "# while decoded_char != \" \" and i < 25:\n",
        "#   i += 1\n",
        "#   # Now, predict using the model\n",
        "#   output_tokens = model.predict([sequence, decoder_input])\n",
        "\n",
        "#   token_index = np.argmax(output_tokens[0, -1, :])\n",
        "\n",
        "#   decoded_char = decoder_tokens[token_index]\n",
        "\n",
        "#   print(decoded_char)\n",
        "\n",
        "#   sequence[0, 0, token_index] = 1.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_word = decode_char(\"K\")\n",
        "\n",
        "print(predicted_word)"
      ],
      "metadata": {
        "id": "59EVitGEgbDr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}