{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEW-SQZoCYbT"
      },
      "source": [
        "# Definición del problema"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se desea entrenar un modelo que sea capaz de 'completar' una palabra a medio escribir, o proponer una corrección para una palabra ya escrita en caso de que la misma se encuentre mal escrita.\n",
        "\n",
        "Se utilizará un algoritmo de 'hallar la palabra incorrecta' para determinar si una palabra está escrita incorrectamente, de acuerdo a un lexicón construido con palabras extraídas de la página web de la RAE (disponible en https://github.com/JorgeDuenasLerin/diccionario-espanol-txt, actualizado en Mayo 2024)\n",
        "\n",
        "Además, se construirá una matriz de probabilidad con las palabras extraídas para que las recomendaciones de completado y corrección se realicen en función de la frecuencia de utilización de las palabras. El sistema será capaz de realizar estas funciones en Español.\n",
        "\n",
        "Se utilizarán textos para entrenarlo."
      ],
      "metadata": {
        "id": "9FcXCHksCbGB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0VZpRHJCYbX"
      },
      "source": [
        "# Estructura del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "ou1izJryCYbY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importacion de Librerias"
      ],
      "metadata": {
        "id": "Zxrx98HjngrH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import keras as kr\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM"
      ],
      "metadata": {
        "id": "R0_I7z3pnlT9"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIQTHR1LCYbZ"
      },
      "source": [
        "# Datos de entrada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "NH6BNWm-CYbZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04270f78-1cd1-48f3-bdf5-334d574a2f4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total line count: 14377\n",
            "Total character count: 2308761\n"
          ]
        }
      ],
      "source": [
        "# Extract text from Spanish book converted into txt format\n",
        "txt_file = \"text_dump.txt\"\n",
        "\n",
        "lines: list[str] = []\n",
        "with open(txt_file, 'r', encoding=\"UTF-8\") as file:\n",
        "  for line in file:\n",
        "    if line != \"\\n\": #Do not include empty lines in text digest\n",
        "      lines.append(line)\n",
        "\n",
        "#As of this point, 'lines' variable should hold the full length of the txt file\n",
        "print(\"Total line count:\", len(lines))\n",
        "print(\"Total character count:\", sum([len(item) for item in lines]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract used vocabulary and construct encode/decode dictionaries\n",
        "full_txt_str: str = \"\"\n",
        "vocab: list[str] = []\n",
        "acc = 0 #Used to print partial progress of the operation\n",
        "big_acc = 1 #Same as above\n",
        "for paragraph in lines[31:14346]: #Do not consider index, acknowledgements, appendix, etc etc (i.e. only consider main story block for training)\n",
        "  full_txt_str += paragraph\n",
        "  for char in paragraph:\n",
        "    if char not in vocab:\n",
        "      vocab.append(char)\n",
        "  #Print partial progress\n",
        "  acc += 1\n",
        "  if (acc >= (14346-31)*0.2):\n",
        "    print(f\"Vocabulary {20*big_acc}% built\")\n",
        "    acc = 0\n",
        "    big_acc += 1\n",
        "vocab.sort()\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "encode_keys:dict[str,int] = {}\n",
        "decode_keys: dict[int, str] = {}\n",
        "for index, char in enumerate(vocab):\n",
        "  encode_keys[char] = index\n",
        "  decode_keys[index] = char\n",
        "\n",
        "#As of this point, the full vocabulary should be indexed\n",
        "print(\"Total unique characters:\", vocab_size)\n",
        "print(\"Total encode/decode dictionary keys:\", len(encode_keys.keys()),\"|\", len(decode_keys.keys()))\n",
        "print(\"Key list\\n\",encode_keys.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzR18gLGE5B0",
        "outputId": "40de53dc-e73a-4264-e40a-2a5b559fe511"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary 20% built\n",
            "Vocabulary 40% built\n",
            "Vocabulary 60% built\n",
            "Vocabulary 80% built\n",
            "Vocabulary 100% built\n",
            "Total unique characters: 97\n",
            "Total encode/decode dictionary keys: 97 | 97\n",
            "Key list\n",
            " dict_keys(['\\n', ' ', '!', '(', ')', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '¡', '«', '»', '¿', 'Á', 'É', 'Í', 'Ñ', 'Ó', 'Ú', 'á', 'é', 'í', 'ñ', 'ó', 'ö', 'ú', 'ü', '—', '’', '“', '”', '…'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct input/output data for LSTM network\n",
        "seq_length = 5\n",
        "input_seqs: list[list[float]] = []\n",
        "output_seqs: list[float] = []\n",
        "acc = 0 #Used to print partial progress of the operation\n",
        "big_acc = 1 #Same as above\n",
        "\n",
        "current:list[float] = []\n",
        "for char in full_txt_str:\n",
        "  if (len(current) >= seq_length):\n",
        "    input_seqs.append(current.copy())\n",
        "    output_seqs.append(encode_keys[char])\n",
        "    current = current[1:] #Remove first element\n",
        "  current.append(encode_keys[char])\n",
        "  #Print partial progress\n",
        "  acc += 1\n",
        "  if (acc >= len(full_txt_str)*0.2):\n",
        "    print(f\"IO data {20*big_acc}% built\")\n",
        "    acc = 0\n",
        "    big_acc += 1\n",
        "\n",
        "#Normalize input sequences\n",
        "normal_input_seqs: list[list[float]] = []\n",
        "for row in input_seqs:\n",
        "  normal_input_seqs.append([item/(vocab_size - 1) for item in row])\n",
        "print(\"Data normalization done\")\n",
        "\n",
        "#Construct arrays from data used\n",
        "train_data: np.ndarray = np.array([np.array(row) for row in normal_input_seqs])\n",
        "train_tags: np.ndarray = np.array([np.zeros(vocab_size) for item in output_seqs])\n",
        "for index, value in enumerate(output_seqs):\n",
        "  train_tags[index][value] = 1.0\n",
        "\n",
        "#As of this point, the full set of training data has been built\n",
        "print(\"Train shape:\", train_data.shape)\n",
        "print(\"Tags shape:\", train_tags.shape)\n",
        "print(\"First 20 steps\")\n",
        "acc = 0\n",
        "for data,tag in zip(train_data, train_tags):\n",
        "  print(acc+1)\n",
        "  print(data)\n",
        "  print(tag)\n",
        "  acc += 1\n",
        "  if (acc >= 20):\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "w3qZR9S7khoH",
        "outputId": "0c0639d7-ae83-4bca-f5dc-3d5c5c648b6e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IO data 20% built\n",
            "IO data 40% built\n",
            "IO data 60% built\n",
            "IO data 80% built\n",
            "Data normalization done\n",
            "Train shape: (2294626, 5)\n",
            "Tags shape: (2294626, 97)\n",
            "First 20 steps\n",
            "1\n",
            "[0.33333333 0.5        0.61458333 0.5        0.60416667]\n",
            "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0.]\n",
            "2\n",
            "[0.5        0.61458333 0.5        0.60416667 0.01041667]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0.]\n",
            "3\n",
            "[0.61458333 0.5        0.60416667 0.01041667 0.67708333]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0.]\n",
            "4\n",
            "[0.5        0.60416667 0.01041667 0.67708333 0.64583333]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0.]\n",
            "5\n",
            "[0.60416667 0.01041667 0.67708333 0.64583333 0.53125   ]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0.]\n",
            "6\n",
            "[0.01041667 0.67708333 0.64583333 0.53125    0.54166667]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0.]\n",
            "7\n",
            "[0.67708333 0.64583333 0.53125    0.54166667 0.91666667]\n",
            "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0.]\n",
            "8\n",
            "[0.64583333 0.53125    0.54166667 0.91666667 0.01041667]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0.]\n",
            "9\n",
            "[0.53125    0.54166667 0.91666667 0.01041667 0.70833333]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0.]\n",
            "10\n",
            "[0.54166667 0.91666667 0.01041667 0.70833333 0.63541667]\n",
            "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0.]\n",
            "11\n",
            "[0.91666667 0.01041667 0.70833333 0.63541667 0.01041667]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0.]\n",
            "12\n",
            "[0.01041667 0.70833333 0.63541667 0.01041667 0.65625   ]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0.]\n",
            "13\n",
            "[0.70833333 0.63541667 0.01041667 0.65625    0.67708333]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0.]\n",
            "14\n",
            "[0.63541667 0.01041667 0.65625    0.67708333 0.64583333]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0.]\n",
            "15\n",
            "[0.01041667 0.65625    0.67708333 0.64583333 0.625     ]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0.]\n",
            "16\n",
            "[0.65625    0.67708333 0.64583333 0.625      0.64583333]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0.]\n",
            "17\n",
            "[0.67708333 0.64583333 0.625      0.64583333 0.63541667]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0.]\n",
            "18\n",
            "[0.64583333 0.625      0.64583333 0.63541667 0.69791667]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0.]\n",
            "19\n",
            "[0.625      0.64583333 0.63541667 0.69791667 0.64583333]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0.]\n",
            "20\n",
            "[0.64583333 0.63541667 0.69791667 0.64583333 0.67708333]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Construcción del modelo"
      ],
      "metadata": {
        "id": "OPUwYDmX97It"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vect_model = Sequential()\n",
        "vect_model.add(kr.Input(shape=(seq_length,1)))\n",
        "vect_model.add(LSTM(10)) #Add dropout?\n",
        "vect_model.add(Dense(vocab_size, activation=\"softmax\"))\n",
        "\n",
        "vect_model.compile(loss=\"categorical_crossentropy\",optimizer=\"rmsprop\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "uBjJAtCr996s"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8irePKGLCYba"
      },
      "source": [
        "# Entrenamiento del modelo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#data_size = (train_data.shape)[0]\n",
        "vect_model.fit(train_data, train_tags, batch_size = 1024,epochs = 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62DcwYfoECfY",
        "outputId": "b47c84a5-8bfb-4e15-f87d-7ceb24e38c7f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m2241/2241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.1415 - loss: 3.3605\n",
            "Epoch 2/10\n",
            "\u001b[1m2241/2241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.1733 - loss: 2.9899\n",
            "Epoch 3/10\n",
            "\u001b[1m2241/2241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 4ms/step - accuracy: 0.1837 - loss: 2.8962\n",
            "Epoch 4/10\n",
            "\u001b[1m2241/2241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.1873 - loss: 2.8587\n",
            "Epoch 5/10\n",
            "\u001b[1m2241/2241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.1933 - loss: 2.8347\n",
            "Epoch 6/10\n",
            "\u001b[1m2241/2241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.1966 - loss: 2.8183\n",
            "Epoch 7/10\n",
            "\u001b[1m2241/2241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.1985 - loss: 2.8057\n",
            "Epoch 8/10\n",
            "\u001b[1m2241/2241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.1988 - loss: 2.7949\n",
            "Epoch 9/10\n",
            "\u001b[1m2241/2241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.2005 - loss: 2.7841\n",
            "Epoch 10/10\n",
            "\u001b[1m2241/2241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.2026 - loss: 2.7752\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7f06ce867370>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QD6SIafXCYbb"
      },
      "source": [
        "# Evaluación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "z7UV-B3eCYbb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1d6a5f9-4e5a-4936-a2e8-eb28c283bca1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "NEXT|1| \n",
            "l \n"
          ]
        }
      ],
      "source": [
        "seed = \"l\"\n",
        "result = seed\n",
        "seed_data = [encode_keys[char]/(vocab_size - 1) for char in seed]\n",
        "for i in range(20):\n",
        "  predict_data = np.reshape(np.array(seed_data), (1, len(seed_data), 1))\n",
        "  prediction = vect_model.predict(predict_data)\n",
        "\n",
        "  index = np.argmax(prediction)\n",
        "  next_char = decode_keys[index]\n",
        "  result += next_char\n",
        "  print(\"NEXT\",index,next_char,sep=\"|\")\n",
        "\n",
        "  seed_data.append(encode_keys[char]/(vocab_size - 1))\n",
        "  seed_data = seed_data[1:] #Remove first element to move window along sequence\n",
        "\n",
        "  if ( (next_char == \" \")or(next_char == \"\\n\") ):\n",
        "    break\n",
        "\n",
        "print(result)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}